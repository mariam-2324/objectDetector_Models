# -*- coding: utf-8 -*-
"""MoonDreamObjD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRjVNErLEV5unu1xpBB7WRpRl-NekadK

# **OBJECT DETECTION WITH DESCRIPTION:**ğŸš€
## **MODEL NAME:** moondream2 ğŸŒ…

### Loaded from Huggingface, This vision-language model (moondream2) generate a description for an image.* ğŸ–¼ï¸

## Step 1:ğŸ‘‰Installing the transformers Library ğŸ“¦
"""

!pip install -U transformers

"""## Step 2:ğŸ‘‰Importing Required Libraries ğŸ“š

*   PIL from Pillow Library; this library handles the image input
"""

from PIL import Image

"""## Step 3:ğŸ‘‰Loading the Model (First Instance) ğŸ§ 
* Loads the pre-trained moondream2 model from the Hugging Face model hub.
* The model is identified by "vikhyatk/moondream2", a specific vision-language model designed to process images and text. ğŸ“·
* trust_remote_code=True allows the execution of custom code from the modelâ€™s repository, which may be required for non-standard models like moondream2. âš ï¸
* torch_dtype="auto" lets PyTorch automatically select the appropriate data type (e.g., float32 or float16) based on the hardware (CPU/GPU). âš¡
"""

# Load model directly
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

model = AutoModelForCausalLM.from_pretrained("vikhyatk/moondream2", trust_remote_code=True, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained("vikhyatk/moondream2")

"""### Step 4:ğŸ‘‰Loading the Image ğŸ–¼ï¸
* Uses PIL to open an image file named tiger.jpg located at the path /content/tiger.jpg (common in Google Colab environments).
* The image is loaded into memory as a PIL.Image object for processing by the model. ğŸ“‚



"""

# Example usage: Load an image and generate a description
image = Image.open("/content/tiger.jpg") # Replace with your image path

enc_image = model.encode_image(image)
caption = model.answer_question(enc_image, "Describe this image.", tokenizer)
print(caption)