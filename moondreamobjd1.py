# -*- coding: utf-8 -*-
"""MoonDreamObjD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bRjVNErLEV5unu1xpBB7WRpRl-NekadK

# **OBJECT DETECTION WITH DESCRIPTION:**🚀
## **MODEL NAME:** moondream2 🌅

### Loaded from Huggingface, This vision-language model (moondream2) generate a description for an image.* 🖼️

## Step 1:👉Installing the transformers Library 📦
"""

!pip install -U transformers

"""## Step 2:👉Importing Required Libraries 📚

*   PIL from Pillow Library; this library handles the image input
"""

from PIL import Image

"""## Step 3:👉Loading the Model (First Instance) 🧠
* Loads the pre-trained moondream2 model from the Hugging Face model hub.
* The model is identified by "vikhyatk/moondream2", a specific vision-language model designed to process images and text. 📷
* trust_remote_code=True allows the execution of custom code from the model’s repository, which may be required for non-standard models like moondream2. ⚠️
* torch_dtype="auto" lets PyTorch automatically select the appropriate data type (e.g., float32 or float16) based on the hardware (CPU/GPU). ⚡
"""

# Load model directly
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image

model = AutoModelForCausalLM.from_pretrained("vikhyatk/moondream2", trust_remote_code=True, torch_dtype="auto")
tokenizer = AutoTokenizer.from_pretrained("vikhyatk/moondream2")

"""### Step 4:👉Loading the Image 🖼️
* Uses PIL to open an image file named tiger.jpg located at the path /content/tiger.jpg (common in Google Colab environments).
* The image is loaded into memory as a PIL.Image object for processing by the model. 📂



"""

# Example usage: Load an image and generate a description
image = Image.open("/content/tiger.jpg") # Replace with your image path

enc_image = model.encode_image(image)
caption = model.answer_question(enc_image, "Describe this image.", tokenizer)
print(caption)